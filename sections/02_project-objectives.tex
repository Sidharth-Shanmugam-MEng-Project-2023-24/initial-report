A dive into machine vision technologies is a requirement to achieve the first research aim, the autonomous segmentation of backscatter. The system must be able to accurately and reliably pinpoint the location of backscatter particles as well as map out the shape of the outline, enabling the perfect segmentation of each particle for elimination. Whilst driving a specialised light source projector, the system must utilise the particle centre location and outline mapping to project holes in the light beam in addition to the implementation of position calibration, which is crucial to minimise the parallax effect, the displacement of the target from the perceived position in the video capture, ultimately ensuring accurate backscatter elimination.

When trying to achieve a system for autonomous backscatter particle segmentation, it is worth noting the vast range of machine vision-based technologies to harness, both in combination or in isolation, each with differing use cases and potential trade-offs. For example, the work of the previous student on this project \cite{katieshepherdMachineVisionBased2023} outlines the backscatter detection and elimination system revolving around a simple blob detection algorithm. However, much industry-related literature refers to an edge-detection algorithm as the industry standard for bubble detection and characterisation systems for underwater environmental analysis. While both of these machine vision-related options are viable solutions to this system, each has differing trade-offs and characteristics which need exploration, and more technologies exist to enhance the system further, requiring additional research.

The work of the previous student on this project discusses the hindrance to the successful operation of the blob detection-based backscatter elimination software due to the effects of the Linux operating system (OS): Linux's ability to run tasks in the background while the program is running, interfered with the execution of the program, making the camera image buffer and freeze \cite{katieshepherdMachineVisionBased2023}. A more controllable OS-level task-switching prioritisation functionality is vital to reducing system latency, thus boosting the performance of the backscatter elimination implementation. Unlike a general-purpose OS such as Linux, a real-time OS (RTOS) ensures compliance with stringent and fixed timing specifications to provide a highly deterministic reaction to any external event.

An RTOS provides an apparent solution to the jitter issues experienced by Shepherd \cite{katieshepherdMachineVisionBased2023} from a non-deterministic response handling by the general-purpose Linux OS. However, an RTOS implementation is more common in specialised embedded systems dedicated to a single functionality, such as single-core microcontrollers. Therefore, there will be a severe system simplicity trade-off when implementing an RTOS due to the dissimilarities of involved hardware and software architectures. Exploration of these trade-offs is crucial to improve the predictability and stability of the backscatter elimination system performance, with the ultimate goal being to strike a balance between these improvements without sacrificing system simplicity by requiring the engineering of low-level customised microcontroller implementations.

High-resolution images consist of high dimensionality with tremendous number of pixels, each containing numerous colour channels and meta-data. Applying machine vision technologies to these images can prove a monumental task to process the vast dataset for even the most perceivably simple functions such as edge detection, segmentation, or filtering. In many cases, machine vision algorithms must consider the entire image data rather than a small, finite subset of pixel values within the image to process frames. Although reducing the resolution of frame captures is viable to reduce the computational overheads, it often trades off object detection and segmentation accuracy. A predictive system approach can mitigate the computationally intensive requirement of machine vision application to every frame of an input video feed.

A system that can accurately predict backscatter particle movement and future locations eliminates the requirement to apply machine vision-based technologies to every frame in the video feed to identify and segment the particles. There are two main approaches to consider when working towards a predictive system: (a) an artificial intelligence (AI) approach, harnessing supervised machine learning (ML) methods with a potential to incorporate unsupervised methods for greater accuracy, or (b) a non-AI/ML interpolative approach with an application of either a linear or polynomial interpolation algorithm to predict future positions concerning the tracked locations in a finite count of previous frames. Again, there are significant trade-offs concerning accuracy, speed, and computational overheads for each approach that requires exploration.
